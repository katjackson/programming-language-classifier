{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data\n",
    "Used glob to read a bunch of files with a similar path according to file extension. This allowed me to get just the languages I wanted in my data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_prog_files(loc):\n",
    "    files = glob.glob(loc, recursive=True)\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in each type of file and concatenated data into x and y lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c texts with file extension gcc: 58\n",
      "c texts with file extension c: 1\n",
      "c# texts with file extension csharp: 41\n",
      "common lisp texts with file extension sbcl: 34\n",
      "clojure texts with file extension clojure: 38\n",
      "java texts with file extension java: 51\n",
      "javascript texts with file extension javascript: 25\n",
      "ocaml texts with file extension ocaml: 34\n",
      "perl texts with file extension perl: 34\n",
      "php texts with file extension hack: 26\n",
      "php texts with file extension php: 29\n",
      "python texts with file extension python3: 36\n",
      "ruby texts with file extension jruby: 34\n",
      "ruby texts with file extension yarv: 39\n",
      "scala texts with file extension scala: 43\n",
      "scheme texts with file extension racket: 29\n",
      "\n",
      "\n",
      "number of texts 552\n",
      "number of targets 552\n",
      "number of potential targets 13\n"
     ]
    }
   ],
   "source": [
    "file_extensions = ['gcc', 'c', 'csharp', 'sbcl', 'clojure', 'java', 'javascript', 'ocaml', 'perl', 'hack', 'php', 'python3', 'jruby', 'yarv', 'scala', 'racket']\n",
    "language_by_extension = {\n",
    "    'jruby': 'ruby',\n",
    "    'csharp': 'c#',\n",
    "    'hack': 'php',\n",
    "    'sbcl': 'common lisp',\n",
    "    'ocaml': 'ocaml',\n",
    "    'python3': 'python',\n",
    "    'php': 'php',\n",
    "    'perl': 'perl',\n",
    "    'racket': 'scheme',\n",
    "    'c': 'c',\n",
    "    'javascript': 'javascript',\n",
    "    'gcc': 'c',\n",
    "    'yarv': 'ruby',\n",
    "    'java': 'java',\n",
    "    'clojure': 'clojure',\n",
    "    'scala': 'scala'\n",
    "}\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for ext in file_extensions:\n",
    "    x_texts = read_prog_files('/Users/kathrynjackson/Code/iron-yard/homework/programming-language-classifier/benchmarksgame-2014-08-31/benchmarksgame/bench/**/*.{}'.format(ext))\n",
    "    X += x_texts\n",
    "    y += (len(x_texts) * [ext_dict[ext]])\n",
    "    print(\"{} texts with file extension {}: {}\".format(language_by_extension[ext], ext, len(x_texts)))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"number of texts\", len(X))\n",
    "print(\"number of targets\", len(y))\n",
    "print(\"number of potential targets\", len(set(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "Used the train_test_split method from sklearn to split data set into 60/40 for training and testing the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, train_size=0.6, random_state=890)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Count Vectorizer\n",
    "I used scikit-learn's count vectorizer to extract features from the data. I wanted words, white spaces, and puctuaion, but not numbers, which I assumed would be less language specific and more project specific. There are a lot of features in this model. I trained several models with this set of features. Each model's score represents its mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features:  5860\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(token_pattern=r'[a-zA-Z]{2,}|\\s|[^\\w\\d\\s]+')\n",
    "cv.fit(X_train)\n",
    "cv.transform(X_train)\n",
    "print(\"number of features: \", len(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.927601809955\n"
     ]
    }
   ],
   "source": [
    "baye_pipe = Pipeline([('vectorizer', CountVectorizer(token_pattern=r'[a-zA-Z]{2,}|\\s|[^\\w\\d\\s]')),\n",
    "                      ('classifier', MultinomialNB())])\n",
    "\n",
    "baye_pipe.fit(X_train, y_train)\n",
    "baye_pipe.named_steps['vectorizer'].transform(X_train)\n",
    "\n",
    "score = baye_pipe.score(X_test, y_test)\n",
    "print(\"mean accuracy: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "The accuracy of this model varies, but stays around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.89592760181\n"
     ]
    }
   ],
   "source": [
    "tree_pipe = Pipeline([('vectorizer', CountVectorizer(token_pattern=r'[a-zA-Z]{2,}|\\s|[^\\w\\d\\s]')),\n",
    "                      ('transformer', TfidfTransformer()),\n",
    "                      ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "tree_pipe.fit(X_train, y_train)\n",
    "tree_pipe.named_steps['vectorizer'].transform(X_train)\n",
    "\n",
    "score = tree_pipe.score(X_test, y_test)\n",
    "print(\"mean accuracy: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "This meta estimator is the most accurate I tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.972850678733\n"
     ]
    }
   ],
   "source": [
    "forest_pipe = Pipeline([('vectorizer', CountVectorizer(token_pattern=r'[a-zA-Z]{2,}|\\s|[^\\w\\d\\s]')),\n",
    "#                       ('transformer', TfidfTransformer()),\n",
    "                        ('classifier', RandomForestClassifier())])\n",
    "\n",
    "forest_pipe.fit(X_train, y_train)\n",
    "forest_pipe.named_steps['vectorizer'].transform(X_train)\n",
    "\n",
    "score = forest_pipe.score(X_test, y_test)\n",
    "print(\"mean accuracy: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "I read in the test files using the same function used earlier. All three classifiers performed poorly on the test data. The model that performed the best in training performed the worst with the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test files stored in test_X\n",
    "test_X = []\n",
    "test_files = list(range(1,33))\n",
    "\n",
    "for file in test_files:\n",
    "    test = read_prog_files('/Users/kathrynjackson/Code/iron-yard/homework/assignments-master/week5/polyglot/test/{}'.format(file))\n",
    "    test_X += test\n",
    "\n",
    "# test targets stored in test_y\n",
    "test_y = []\n",
    "with open('/Users/kathrynjackson/Code/iron-yard/homework/assignments-master/week5/polyglot/test.csv') as test_targets:\n",
    "    lines = csv.reader(test_targets)\n",
    "    for line in lines:\n",
    "        test_y.append(line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.5625 \n",
      "\n",
      "PREDICTION :: ACTUAL\n",
      "scheme :: clojure\n",
      "javascript :: clojure\n",
      "javascript :: clojure\n",
      "scheme :: clojure\n",
      "python :: python\n",
      "python :: python\n",
      "javascript :: python\n",
      "python :: python\n",
      "javascript :: javascript\n",
      "javascript :: javascript\n",
      "javascript :: javascript\n",
      "javascript :: javascript\n",
      "ruby :: ruby\n",
      "ruby :: ruby\n",
      "ruby :: ruby\n",
      "java :: haskell\n",
      "javascript :: haskell\n",
      "php :: haskell\n",
      "javascript :: scheme\n",
      "javascript :: scheme\n",
      "scheme :: scheme\n",
      "java :: java\n",
      "java :: java\n",
      "php :: scala\n",
      "javascript :: scala\n",
      "php :: tcl\n",
      "php :: tcl\n",
      "php :: php\n",
      "php :: php\n",
      "php :: php\n",
      "ocaml :: ocaml\n",
      "ocaml :: ocaml\n"
     ]
    }
   ],
   "source": [
    "score = tree_pipe.score(testy_X, testy_y)\n",
    "prediction = tree_pipe.predict(testy_X)\n",
    "\n",
    "print(\"mean accuracy: \", score, \"\\n\")\n",
    "print(\"PREDICTION :: ACTUAL\")\n",
    "for pair in zip(prediction, test_y):\n",
    "    print(\"{} :: {}\".format(pair[0], pair [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.28125 \n",
      "\n",
      "PREDICTION :: ACTUAL\n",
      "ocaml :: clojure\n",
      "ruby :: clojure\n",
      "ruby :: clojure\n",
      "ruby :: clojure\n",
      "ocaml :: python\n",
      "python :: python\n",
      "ruby :: python\n",
      "ruby :: python\n",
      "javascript :: javascript\n",
      "ruby :: javascript\n",
      "scala :: javascript\n",
      "javascript :: javascript\n",
      "ruby :: ruby\n",
      "ruby :: ruby\n",
      "ruby :: ruby\n",
      "ruby :: haskell\n",
      "ruby :: haskell\n",
      "perl :: haskell\n",
      "javascript :: scheme\n",
      "javascript :: scheme\n",
      "common lisp :: scheme\n",
      "javascript :: java\n",
      "javascript :: java\n",
      "javascript :: scala\n",
      "scala :: scala\n",
      "php :: tcl\n",
      "javascript :: tcl\n",
      "php :: php\n",
      "javascript :: php\n",
      "javascript :: php\n",
      "ocaml :: ocaml\n",
      "javascript :: ocaml\n"
     ]
    }
   ],
   "source": [
    "score = forest_pipe.score(testy_X, testy_y)\n",
    "prediction = forest_pipe.predict(testy_X)\n",
    "\n",
    "print(\"mean accuracy: \", score, \"\\n\")\n",
    "print(\"PREDICTION :: ACTUAL\")\n",
    "for pair in zip(prediction, test_y):\n",
    "    print(\"{} :: {}\".format(pair[0], pair [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  0.65625 \n",
      "\n",
      "PREDICTION :: ACTUAL\n",
      "clojure :: clojure\n",
      "clojure :: clojure\n",
      "clojure :: clojure\n",
      "java :: clojure\n",
      "python :: python\n",
      "python :: python\n",
      "python :: python\n",
      "python :: python\n",
      "javascript :: javascript\n",
      "javascript :: javascript\n",
      "scala :: javascript\n",
      "scala :: javascript\n",
      "ruby :: ruby\n",
      "ruby :: ruby\n",
      "ruby :: ruby\n",
      "java :: haskell\n",
      "scala :: haskell\n",
      "scala :: haskell\n",
      "scheme :: scheme\n",
      "scheme :: scheme\n",
      "scheme :: scheme\n",
      "c :: java\n",
      "c :: java\n",
      "scala :: scala\n",
      "scala :: scala\n",
      "php :: tcl\n",
      "php :: tcl\n",
      "c :: php\n",
      "php :: php\n",
      "php :: php\n",
      "ocaml :: ocaml\n",
      "ocaml :: ocaml\n"
     ]
    }
   ],
   "source": [
    "score = baye_pipe.score(testy_X, testy_y)\n",
    "prediction = baye_pipe.predict(testy_X)\n",
    "\n",
    "print(\"mean accuracy: \", score, \"\\n\")\n",
    "print(\"PREDICTION :: ACTUAL\")\n",
    "for pair in zip(prediction, test_y):\n",
    "    print(\"{} :: {}\".format(pair[0], pair [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Own\n",
    "I used several methods to try to improve the classifier, but it's still not very good. First, I built a custom featurizer using the following functions. I ended up writing a function that takes a regular expression so that I could try different things quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def caps_to_non(text):\n",
    "    cap_letters = re.findall(r'[A-Z]', text)\n",
    "    non_caps = re.findall(r'[a-z]', text)\n",
    "    return len(cap_letters) / len(non_caps)\n",
    "    \n",
    "    \n",
    "def percent_occurence_of_parenthesis(text):\n",
    "    pars = re.findall(r'\\(|\\)', text)\n",
    "    return len(pars) / len(text)\n",
    "\n",
    "\n",
    "def percent_occurence_of_curly(text):\n",
    "    curls = re.findall(r'\\{|\\}', text)\n",
    "    return len(curls) / len(text)\n",
    "\n",
    "\n",
    "def percent_occurence_of_space(text):\n",
    "    spaces = re.findall(r'\\s', text)\n",
    "    return len(spaces) / len(text)\n",
    "\n",
    "    \n",
    "def occurence_of_this_pattern(reg_ex):\n",
    "    \n",
    "    def feature_fn(text):\n",
    "        occ = re.findall(r'{}'.format(reg_ex), text)\n",
    "        return len(occ)\n",
    "\n",
    "    return feature_fn\n",
    "\n",
    "\n",
    "class FunctionFeaturizer(TransformerMixin):\n",
    "    def __init__(self, *featurizers):\n",
    "        self.featurizers = featurizers\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        fvs = []\n",
    "        for text in X:\n",
    "            fv = [f(text) for f in self.featurizers]\n",
    "            fvs.append(fv)\n",
    "        return np.array(fvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After instantiating my featurizer class, I combined it with the CountVectorizer using the sklearn class FeatureUnion. I passed a regular expression to the count vectorizer that tokenizes words, all kinds of white space, and different punctuation. I am using the decision tree classifier, which scored highest in my previous trials, in addition to the linear support vector classifier. Using the TfidfTransformer seems to make the score worse.<br>\n",
    "<br>\n",
    "Without random state argurments, the resulting classifier predicts correctly anywhere from 3.5% to 15.6% of the time, but usually hits between 9% - 12.5%. I don't have any other ideas for how to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "mean accuracy:  0.868778280543 \n",
      "\n",
      "TESTING\n",
      "mean accuracy:  0.5625 \n",
      "\n",
      "PREDICTION :: ACTUAL\n",
      "clojure :: clojure\n",
      "clojure :: clojure\n",
      "clojure :: clojure\n",
      "java :: clojure\n",
      "ruby :: python\n",
      "python :: python\n",
      "python :: python\n",
      "python :: python\n",
      "javascript :: javascript\n",
      "javascript :: javascript\n",
      "scala :: javascript\n",
      "scala :: javascript\n",
      "ruby :: ruby\n",
      "ruby :: ruby\n",
      "ruby :: ruby\n",
      "java :: haskell\n",
      "clojure :: haskell\n",
      "scala :: haskell\n",
      "common lisp :: scheme\n",
      "scheme :: scheme\n",
      "scheme :: scheme\n",
      "c :: java\n",
      "c :: java\n",
      "scala :: scala\n",
      "scala :: scala\n",
      "php :: tcl\n",
      "php :: tcl\n",
      "c :: php\n",
      "php :: php\n",
      "php :: php\n",
      "c# :: ocaml\n",
      "ocaml :: ocaml\n"
     ]
    }
   ],
   "source": [
    "featurizer = FunctionFeaturizer(caps_to_non,\n",
    "                                percent_occurence_of_parenthesis,\n",
    "                                percent_occurence_of_curly,\n",
    "                                percent_occurence_of_space,\n",
    "                                occurence_of_this_pattern('&\\w'),\n",
    "                                occurence_of_this_pattern('\\$\\w'),\n",
    "                                occurence_of_this_pattern('[A-Za-z]+[A-Z]'))\n",
    "\n",
    "cv = CountVectorizer(token_pattern=r'[a-zA-Z]{2,}|\\s|[^\\w\\d\\s]', lowercase=False)\n",
    "\n",
    "feature_extractors = FeatureUnion([('my featurizer', featurizer), ('cv', cv)])\n",
    "\n",
    "my_classifier = Pipeline([\n",
    "                    ('featurizer', feature_extractors),\n",
    "                    ('transformer', TfidfTransformer()),\n",
    "#                     ('classifier', DecisionTreeClassifier(random_state=1067)),\n",
    "                    ('linsvc', LinearSVC(random_state=13)),\n",
    "                    ])\n",
    "\n",
    "my_classifier.fit(X_train, y_train)\n",
    "my_classifier.named_steps['featurizer'].transform(X_train)\n",
    "train_score = my_classifier.score(X_test, y_test)\n",
    "\n",
    "test_score = my_classifier.score(testy_X, testy_y)\n",
    "prediction = my_classifier.predict(testy_X)\n",
    "\n",
    "print(\"TRAINING\")\n",
    "print(\"mean accuracy: \", train_score, \"\\n\")\n",
    "print(\"TESTING\")\n",
    "print(\"mean accuracy: \", test_score, \"\\n\")\n",
    "print(\"PREDICTION :: ACTUAL\")\n",
    "for pair in zip(prediction, test_y):\n",
    "    print(\"{} :: {}\".format(pair[0], pair [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          c       0.00      0.00      0.00         3\n",
      "         c#       0.00      0.00      0.00         1\n",
      "    clojure       0.75      0.75      0.75         4\n",
      "common lisp       0.00      0.00      0.00         1\n",
      "    haskell       0.00      0.00      0.00         0\n",
      "       java       0.00      0.00      0.00         2\n",
      " javascript       0.50      1.00      0.67         2\n",
      "      ocaml       0.50      1.00      0.67         1\n",
      "        php       0.67      0.50      0.57         4\n",
      "     python       0.75      1.00      0.86         3\n",
      "       ruby       1.00      0.75      0.86         4\n",
      "      scala       1.00      0.40      0.57         5\n",
      "     scheme       0.67      1.00      0.80         2\n",
      "        tcl       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.62      0.56      0.55        32\n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2 0 0 0 0 2 0 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 2 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 3 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 3 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 2 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kathrynjackson/Code/homework/programming-language-classifier/.direnv/python-3.5.1/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/kathrynjackson/Code/homework/programming-language-classifier/.direnv/python-3.5.1/lib/python3.5/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(my_classifier.predict(testy_X), testy_y))\n",
    "print(confusion_matrix(testy_y, my_classifier.predict(testy_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The commented code is full of unsuccessful attempts to modify the CountVectorizer. I tried passing my own specific vocabulary. I tried different regular expressions. I tried analizing 2 and 3 character n-grams rather than words. None of these methods made a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# my_vocab = ['function', '{', '}', '\\n', '\\t', ':', ';', 'def', ',', '->',\n",
    "#             '(', ')', 'call', 'lambda', 'set', '@', '>', '<', '.', '[',\n",
    "#             ']', 'var', 'elif', 'else', 'else if', 'then', 'in',\n",
    "#             'switch', 'IfTrue', 'IfFalse', 'unless', 'not', 'elsif',\n",
    "#             'given', 'end', 'match', '(if', '(otherwise', 'progn', 'begin',\n",
    "#             'cond', 'then begin', 'with', 'when', 'foreach', 'for each',\n",
    "#             'for_each', 'for (', '$i++', '$i', '$', 'do', 'until', 'loop',\n",
    "#             'let loop', 'for-each', 'done', '.iter', 'catch', 'except',\n",
    "#             'longjmp', 'setjmp', 'finally', 'throw', 'die', 'eval', '$@',\n",
    "#             'rescue', 'ensure', 'handler-', 'check-', 'guard', 'try:',\n",
    "#             'catchError', 'last', 'break', 'return-from',\n",
    "#             'loop-finish', 'go', 'goto', 'next', 'func', 'void', 'int main',\n",
    "#             'main', 'public', 'defun', 'setf', 'define', '&', '*', '/',\n",
    "#             'require', ' = ', 'import', '__init__']\n",
    "# cv = CountVectorizer(vocabulary=my_vocab)\n",
    "\n",
    "# cv = CountVectorizer(token_pattern=r'[a-zA-Z]{2,}|\\s|[^\\w\\d\\s]', lowercase=False)\n",
    "# cv = CountVectorizer(token_pattern=r'[a-zA-Z]{2,}|[^\\w\\d\\s]+')\n",
    "\n",
    "# cv = CountVectorizer(analyzer='char', ngram_range=(2,3))\n",
    "\n",
    "# cv = CountVectorizer(lowercase=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_guesser(snippet):\n",
    "    return my_classifier.predict([snippet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['python'], \n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_guesser('''def an_imaginary_function:\\n    return dict = {'a': 'B'}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
